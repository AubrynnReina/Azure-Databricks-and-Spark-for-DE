{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85cfb5b3-99d1-42fa-b17d-94a372aad2d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cfa508b-1e62-4442-9d72-906790dedad4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def add_ingestion_date(input_df):\n",
    "    return input_df.withColumn('ingestion_date', current_timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "329c959b-34c8-47b5-bde5-9f7764f864d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def upsert_to_delta_table(database, table, source_container, source_df, upsert_condition, partition_column):\n",
    "\n",
    "    spark.conf.set('spark.databricks.optimizer.dynamicPartitions', 'true')\n",
    "\n",
    "    from delta.tables import DeltaTable\n",
    "    if spark._jsparkSession.catalog().tableExists(database, table):\n",
    "        delta_table = DeltaTable.forPath(spark, f'{source_container}/{table}')\n",
    "        delta_table.alias('target')\\\n",
    "            .merge(\n",
    "                source_df.alias('source'),\n",
    "                upsert_condition\n",
    "            )\\\n",
    "            .whenMatchedUpdateAll()\\\n",
    "            .whenNotMatchedInsertAll()\\\n",
    "            .execute()\n",
    "    else:\n",
    "        source_df.write\\\n",
    "            .format('delta')\\\n",
    "            .mode('overwrite')\\\n",
    "            .partitionBy(partition_column)\\\n",
    "            .saveAsTable(f'{database}.{table}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "248e5f3c-394c-4917-a29c-9f8e0bcea600",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "common_functions",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
